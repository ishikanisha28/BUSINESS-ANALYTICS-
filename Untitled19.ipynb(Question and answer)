{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled19.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiD9H/vr0apRHUKwQvJDXZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishikanisha28/BUSINESS-ANALYTICS-/blob/main/Untitled19.ipynb(Question%20and%20answer)\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJcK_VgDv1ou",
        "outputId": "5aee17c8-ba63-4d3b-e378-32b09e41fec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "/bin/bash: $: command not found\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install --upgrade transformers sentencepiece\n",
        "!$ pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.2.0/en_core_web_trf-3.2.0-py3-none-any.whl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Importing required packages\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LzSEFw1uv-gg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Load pre-trained Bert model\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "tokenizer_for_bert = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "metadata": {
        "id": "xiy2KRHCwA7d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_question_answer(question, passage, max_len=500):\n",
        "    \n",
        "    \"\"\"\n",
        "    question: What is the name of YouTube Channel\n",
        "    passage: Watch complete playlist of Natural Language Processing. Don't forget to like, share and subscribe my channel IG Tech Team\n",
        "    \"\"\"\n",
        "\n",
        "    #Tokenize input question and passage \n",
        "    #Add special tokens - [CLS] and [SEP]\n",
        "    input_ids = tokenizer_for_bert.encode (question, passage,  max_length= max_len, truncation=True)  \n",
        "    \"\"\"\n",
        "    [101, 2054, 2003, 1996, 2171, 1997, 7858, 3149, 102, 3422, 3143, 2377, 9863, 1997, 3019, 2653, 6364, 1012, \n",
        "    2123, 1005, 1056, 5293, 2000, 2066, 1010, 3745, 1998, 4942, 29234, 2026, 3149, 1045, 2290, 6627, 2136, 102]\n",
        "    \"\"\"\n",
        "    #Getting number of tokens in 1st sentence (question) and 2nd sentence (passage that contains answer)\n",
        "    sep_index = input_ids.index(102) \n",
        "    len_question = sep_index + 1   \n",
        "    len_passage = len(input_ids)- len_question  \n",
        "    \"\"\"\n",
        "    8\n",
        "    9\n",
        "    27\n",
        "    \"\"\"\n",
        "    \n",
        "    #Need to separate question and passage\n",
        "    #Segment ids will be 0 for question and 1 for passage\n",
        "    segment_ids =  [0]*len_question + [1]*(len_passage)  \n",
        "    \"\"\"\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "    \"\"\"\n",
        "     #Converting token ids to tokens\n",
        "    tokens = tokenizer_for_bert.convert_ids_to_tokens(input_ids) \n",
        "    \"\"\"\n",
        "    tokens = ['[CLS]', 'what', 'is', 'the', 'name', 'of', 'youtube', 'channel', '[SEP]', 'watch', 'complete', \n",
        "    'play', '##list', 'of', 'natural', 'language', 'processing', '.', 'don', \"'\", 't', 'forget', 'to', 'like', \n",
        "    ',', 'share', 'and', 'sub', '##scribe', 'my', 'channel', 'i', '##g', 'tech', 'team', '[SEP]']\n",
        "    \"\"\"\n",
        "\n",
        "    #Getting start and end scores for answer\n",
        "    #Converting input arrays to torch tensors before passing to the model\n",
        "    start_token_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]) )[0]\n",
        "    end_token_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]) )[1]\n",
        "    \"\"\"\n",
        "    tensor([[-5.9787, -3.0541, -7.7166, -5.9291, -6.8790, -7.2380, -1.8289, -8.1006,\n",
        "         -5.9786, -3.9319, -5.6230, -4.1919, -7.2068, -6.7739, -2.3960, -5.9425,\n",
        "         -5.6828, -8.7007, -4.2650, -8.0987, -8.0837, -7.1799, -7.7863, -5.1605,\n",
        "         -8.2832, -5.1088, -8.1051, -5.3985, -6.7129, -1.4109, -3.2241,  1.5863,\n",
        "         -4.9714, -4.1138, -5.9107, -5.9786]], grad_fn=<SqueezeBackward1>)\n",
        "    tensor([[-2.1025, -2.9121, -5.9192, -6.7459, -6.4667, -5.6418, -1.4504, -3.1943,\n",
        "         -2.1024, -5.7470, -6.3381, -5.8520, -3.4871, -6.7667, -5.4711, -3.9885,\n",
        "         -1.2502, -4.0869, -6.4930, -6.3751, -6.1309, -6.9721, -7.5558, -6.4056,\n",
        "         -6.7456, -5.0527, -7.3854, -7.0440, -4.3720, -3.8936, -2.1085, -5.8211,\n",
        "         -2.0906, -2.2184,  1.4268, -2.1026]], grad_fn=<SqueezeBackward1>)\n",
        "    \"\"\"\n",
        "    #Converting scores tensors to numpy arrays\n",
        "    start_token_scores = start_token_scores.detach().numpy().flatten()\n",
        "    end_token_scores = end_token_scores.detach().numpy().flatten()\n",
        "    \"\"\"\n",
        "    [-5.978666  -3.0541189 -7.7166095 -5.929051  -6.878973  -7.238004\n",
        "    -1.8289301 -8.10058   -5.9786286 -3.9319289 -5.6229596 -4.191908\n",
        "    -7.20684   -6.773916  -2.3959794 -5.942456  -5.6827617 -8.700695\n",
        "    -4.265001  -8.09874   -8.083673  -7.179875  -7.7863474 -5.16046\n",
        "    -8.283156  -5.108819  -8.1051235 -5.3984528 -6.7128663 -1.4108785\n",
        "    -3.2240815  1.5863497 -4.9714    -4.113782  -5.9107194 -5.9786243]\n",
        "\n",
        "    [-2.1025064 -2.912148  -5.9192414 -6.745929  -6.466673  -5.641759\n",
        "    -1.4504088 -3.1943028 -2.1024144 -5.747039  -6.3380575 -5.852047\n",
        "    -3.487066  -6.7667046 -5.471078  -3.9884708 -1.2501552 -4.0868535\n",
        "    -6.4929943 -6.375147  -6.130891  -6.972091  -7.5557766 -6.405638\n",
        "    -6.7455807 -5.0527067 -7.3854156 -7.043977  -4.37199   -3.8935976\n",
        "    -2.1084964 -5.8210607 -2.0906193 -2.2184045  1.4268283 -2.1025767]\n",
        "    \"\"\"\n",
        "     #Getting start and end index of answer based on highest scores\n",
        "    answer_start_index = np.argmax(start_token_scores)\n",
        "    answer_end_index = np.argmax(end_token_scores)\n",
        "    \"\"\"\n",
        "    31\n",
        "    34\n",
        "    \"\"\"\n",
        "\n",
        "    #Getting scores for start and end token of the answer\n",
        "    start_token_score = np.round(start_token_scores[answer_start_index], 2)\n",
        "    end_token_score = np.round(end_token_scores[answer_end_index], 2)\n",
        "    \"\"\"\n",
        "    1.59\n",
        "    1.43\n",
        "    \"\"\"\n",
        "    #Combining subwords starting with ## and get full words in output. \n",
        "    #It is because tokenizer breaks words which are not in its vocab.\n",
        "    answer = tokens[answer_start_index] \n",
        "    for i in range(answer_start_index + 1, answer_end_index + 1):\n",
        "        if tokens[i][0:2] == '##':  \n",
        "            answer += tokens[i][2:] \n",
        "        else:\n",
        "            answer += ' ' + tokens[i]  \n",
        "\n",
        "    # If the answer didn't find in the passage\n",
        "    if ( answer_start_index == 0) or (start_token_score < 0 ) or  (answer == '[SEP]') or ( answer_end_index <  answer_start_index):\n",
        "        answer = \"Sorry!, I could not find an answer in the passage.\"\n",
        "    \n",
        "    return (answer_start_index, answer_end_index, start_token_score, end_token_score,  answer)\n",
        "\n",
        "#Testing function\n",
        "bert_question_answer(\"What is the name of YouTube Channel\", \"Watch complete playlist of Natural Language Processing. Don't forget to like, share and subscribe my channel IG Tech Team \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmBsUutlwS9K",
        "outputId": "1b2d597e-e686-4fdb-c961-69805af602f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 34, 1.59, 1.43, 'ig tech team')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let me define one passage\n",
        "passage = \"\"\"Hello, I am Ishika. My friend name is Sakshi. He is the son of Pradip. I spend most of the time with Sakshi. \n",
        "He always call me by my nick name. Sakshi call me programmer. Except Sakshi, my other friend call me by my original name. \n",
        "Amrita is also my friend. \"\"\"\n",
        "\n",
        "print (f'Length of the passage: {len(passage.split())} words')\n",
        "\n",
        "question1 =\"What is my name\" \n",
        "print ('\\nQuestion 1:\\n', question1)\n",
        "_, _ , _ , _, ans  = bert_question_answer( question1, passage)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
        "\n",
        "\n",
        "question2 =\"Who is the father of Sakshi\"\n",
        "print ('\\nQuestion 2:\\n', question2)\n",
        "_, _ , _ , _, ans  = bert_question_answer( question2, passage)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
        "\n",
        "question3 =\"With whom Ishika spend most of the time\" \n",
        "print ('\\nQuestion 3:\\n', question3)\n",
        "_, _ , _ , _, ans  = bert_question_answer( question3, passage)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkfweAuswrw3",
        "outputId": "0b3669ce-ad93-410d-fb9a-28e762474072"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the passage: 51 words\n",
            "\n",
            "Question 1:\n",
            " What is my name\n",
            "\n",
            "Answer from BERT:  ishika \n",
            "\n",
            "\n",
            "Question 2:\n",
            " Who is the father of Sakshi\n",
            "\n",
            "Answer from BERT:  pradip \n",
            "\n",
            "\n",
            "Question 3:\n",
            " With whom Ishika spend most of the time\n",
            "\n",
            "Answer from BERT:  sakshi \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let me define another passage\n",
        "passage= \"\"\"NLP is a subfield of computer science and artificial intelligence concerned with interactions between \n",
        "computers and human (natural) languages. It is used to apply machine learning algorithms to text and speech. For \n",
        "example, we can use NLP to create systems like speech recognition, document summarization, machine translation, spam \n",
        "detection, named entity recognition, question answering, autocomplete, predictive typing and so on. Nowadays, most of \n",
        "us have smartphones that have speech recognition. These smartphones use NLP to understand what is said. Also, many \n",
        "people use laptops which operating system has a built-in speech recognition. NLTK (Natural Language Toolkit) is a \n",
        "leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces \n",
        "to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, \n",
        "tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, \n",
        "community-driven project. We’ll use this toolkit to show some basics of the natural language processing field. For \n",
        "the examples below, I’ll assume that we have imported the NLTK toolkit. We can do this like this: import nltk. \n",
        "Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into \n",
        "its component sentences. The idea here looks very simple. Word tokenization (also called word segmentation)\n",
        "is the problem of dividing a string of written language into its component words. In English and many other languages\n",
        "using some form of Latin alphabet, space is a good approximation of a word divider. However, we still can have problems\n",
        "we only split by space to achieve the wanted results. Some English compound nouns are variably written and sometimes\n",
        "they contain a space. In most cases, we use a library to achieve the wanted results, so again don’t worry too much \n",
        "for the details. Stop words are words which are filtered out before or after processing of text. When applying machine\n",
        "learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words.\n",
        "Stop words usually refer to the most common words such as “and”, “the”, “a” in a language, but there is no single\n",
        "universal list of stopwords. The list of the stop words can change depending on your application. The NLTK tool has\n",
        "a predefined list of stopwords that refers to the most common words. If you use it for your first time, you need to\n",
        "download the stop words using this code: nltk.download(“stopwords”). Once we complete the downloading, we can load\n",
        "the stopwords package from the nltk.corpus and use it to load the stop words.\"\"\"\n",
        "\n",
        "print (f'Length of the passage: {len(passage.split())} words')\n",
        "\n",
        "question =\"What is full form of NLTK\"\n",
        "print ('\\nQuestion 1:\\n', question)\n",
        "_, _ , _ , _, ans  = bert_question_answer( question, passage)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
        "\n",
        "question =\"What are stop words \"\n",
        "print ('\\nQuestion 2:\\n', question)\n",
        "_, _ , _ , _, ans  = bert_question_answer( question, passage)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
        "\n",
        "question =\"What is supervised learning\"\n",
        "print ('\\nQuestion 7:\\n', question)\n",
        "_, _ , _ , _, ans  = bert_question_answer( question, passage)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJDpx2CBxdNm",
        "outputId": "2a9da470-8bf9-48c3-cfdb-234e1b5803e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the passage: 433 words\n",
            "\n",
            "Question 1:\n",
            " What is full form of NLTK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer from BERT:  natural language toolkit \n",
            "\n",
            "\n",
            "Question 2:\n",
            " What are stop words \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer from BERT:  words which are filtered out before or after processing of text \n",
            "\n",
            "\n",
            "Question 7:\n",
            " What is supervised learning\n",
            "\n",
            "Answer from BERT:  Sorry!, I could not find an answer in the passage. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Final Question Answering application:\n",
        "#@title Question-Answering Application { vertical-output: true }\n",
        "#@markdown ---\n",
        "question= \"what is NLP\" #@param {type:\"string\"}\n",
        "passage = \"\"\"NLP is a subfield of computer science and artificial intelligence concerned with interactions between \n",
        "computers and human (natural) languages. It is used to apply machine learning algorithms to text and speech. For \n",
        "example, we can use NLP to create systems like speech recognition, document summarization, machine translation, spam \n",
        "detection, named entity recognition, question answering, autocomplete, predictive typing and so on. Nowadays, most of \n",
        "us have smartphones that have speech recognition. These smartphones use NLP to understand what is said. Also, many \n",
        "people use laptops which operating system has a built-in speech recognition. NLTK (Natural Language Toolkit) is a \n",
        "leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces \n",
        "to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, \n",
        "tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, \n",
        "community-driven project. We’ll use this toolkit to show some basics of the natural language processing field. For \n",
        "the examples below, I’ll assume that we have imported the NLTK toolkit. We can do this like this: import nltk. \n",
        "Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into \n",
        "its component sentences. The idea here looks very simple. Word tokenization (also called word segmentation)\n",
        "is the problem of dividing a string of written language into its component words. In English and many other languages\n",
        "using some form of Latin alphabet, space is a good approximation of a word divider. However, we still can have problems\n",
        "we only split by space to achieve the wanted results. Some English compound nouns are variably written and sometimes\n",
        "they contain a space. In most cases, we use a library to achieve the wanted results, so again don’t worry too much \n",
        "for the details. Stop words are words which are filtered out before or after processing of text. When applying machine\n",
        "learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words.\n",
        "Stop words usually refer to the most common words such as “and”, “the”, “a” in a language, but there is no single\n",
        "universal list of stopwords. The list of the stop words can change depending on your application. The NLTK tool has\n",
        "a predefined list of stopwords that refers to the most common words. If you use it for your first time, you need to\n",
        "download the stop words using this code: nltk.download(“stopwords”). Once we complete the downloading, we can load\n",
        "the stopwords package from the nltk.corpus and use it to load the stop words.\"\"\"\n",
        "#@markdown ---\n",
        "_, _ , _ , _, ans  = bert_question_answer( question, passage)\n",
        "\n",
        "#@markdown Answer:\n",
        "print(ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufZYM29cyKvS",
        "outputId": "b4504241-5ec1-46e8-87e7-65eca84cfdcc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a subfield of computer science and artificial intelligence concerned with interactions between computers and human ( natural ) languages\n"
          ]
        }
      ]
    }
  ]
}